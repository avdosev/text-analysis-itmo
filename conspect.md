
## Нейронный сети
###  Word2Vec

вектор - веса ембединга

### FastText

подход похожий на word2vec, но теперь токен не целое слово, какая то часть (как в byte-pair encoding)

### Обучение нейронок

было про полносвязнные сети

Проблемы
* переобучение
* затухающий градиент

Переобучение:
* смотреть на функцию оценки на тестовом датасете и остановить когда пошли вверх (или запоминать последний лок. минимум)

| train | dev | test |
| --- | --- | --- |
| 70% | 20% | 10% |

* можно отследить по весам

когда модель переобучается оказывается, что веса очень быстро разрастаются

модель становится численно неустойчивой

Решение:

можно обучать модель стремясь  сохранить веса минимальными - `регуляризация`

l2-регуляризация

$$ argmin [Q(X^{e}, w) + \lambda_{2} * sum(W_{i}^2)] $$

l1-регуляризация

$$ argmin [Q(X^{e}, w) + \lambda_{2} * sum(|W_{i}|)] $$

особенность в том, что l1 стремится сильнее занулять

`dropout` - случайное зануление нейрона \
`dropconnect` - зануление отдельных связей

