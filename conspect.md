
## Нейронный сети
###  Word2Vec

вектор - веса ембединга

### FastText

подход похожий на word2vec, но теперь токен не целое слово, какая то часть (как в byte-pair encoding)

### Обучение нейронок

было про полносвязнные сети

Проблемы
* переобучение
* затухающий градиент

Переобучение:
* смотреть на функцию оценки на тестовом датасете и остановить когда пошли вверх (или запоминать последний лок. минимум)

| train | dev | test |
| --- | --- | --- |
| 70% | 20% | 10% |

* можно отследить по весам

когда модель переобучается оказывается, что веса очень быстро разрастаются

модель становится численно неустойчивой

Решение:

можно обучать модель стремясь  сохранить веса минимальными - `регуляризация`

l2-регуляризация

$ argmin [Q(X^{e}, w) + \lambda_{2} * sum(W_{i}^2)] $

l1-регуляризация

$ argmin [Q(X^{e}, w) + \lambda_{2} * sum(|W_{i}|)] $

особенность в том, что l1 стремится сильнее занулять

`dropout` - случайное зануление нейрона \
`dropconnect` - зануление отдельных связей \
`batchnorm` 

Техники
1. Full gradient descent
2. SGD - stohastic grad descent
3. Batch grad. descent

### CNN, RNN

Проблемы которые решает CNN

1. Восприятие геометрии текста 

Допустим, есть ff модель

`Benj(i)o`

На вход one-hot encoding

Далее один общий слой для каждого слова подавался  отдельно

Конкатенация

Softmax

transfer learning / fine tuning - решаем одну задачу затем меняем "голову" дообучаем и более сложная задача обучается быстрее

`DSSM` \
Ronan Kolagert \
Deep NN with Multitask learning

Рекурентные нейронные сети

### Трансформеры

```
input   | -> transformers -> | output
t1...tn |                    | t1...tm
```

transformer:
```
                Output
   (embeding)     |
    Encoder ->  Decoder
       |
    Input [Size:512]
```

ембединги контексто-зависимые


encoder:

```
token -> self-attention -> ff -> merge -> embeding
  \                              /
   -----------------------------
```

self-attention

мне впадлу рисовать

Подходы обучения
1. transfer learning
2. few-short learning
3. zero-short learning

## инфопоиск

